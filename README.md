ğŸ  Experimento de RegressÃ£o - House Prices

Este repositÃ³rio contÃ©m um experimento de Aprendizado de MÃ¡quina aplicado ao conjunto de dados Ames Housing Dataset, com o objetivo de prever o preÃ§o de imÃ³veis a partir de suas caracterÃ­sticas.

------------------------------------------------------------
ğŸ¯ Objetivo

Avaliar e comparar o desempenho de diferentes algoritmos de regressÃ£o na tarefa de prediÃ§Ã£o de preÃ§os de imÃ³veis. 
O experimento busca identificar o modelo que oferece o melhor equilÃ­brio entre erro e capacidade preditiva.

------------------------------------------------------------
ğŸ“¦ Dataset

â€¢ Fonte: Kaggle â€“ Ames Housing Dataset
â€¢ Tamanho: ~500 amostras  
â€¢ DescriÃ§Ã£o: Inclui variÃ¡veis como Ã¡rea construÃ­da, nÃºmero de quartos, banheiros, localizaÃ§Ã£o, entre outras.  
â€¢ VariÃ¡vel alvo: PreÃ§o de venda

------------------------------------------------------------
âš™ï¸ Modelos testados

| Modelo                     | RMSE     | MAE     | RÂ²     |
| -------------------------- | -------- | ------- | ------ |
| **XGBoost (Optuna)**       | 5.46e+08 | 13739.2 | 0.9319 |
| **XGBoost**                | 6.86e+08 | 15535.0 | 0.9145 |
| **Random Forest**          | 7.11e+08 | 15906.6 | 0.9113 |
| **Random Forest (Optuna)** | 7.27e+08 | 15879.7 | 0.9093 |
| **Ridge**                  | 8.27e+08 | 16158.6 | 0.8968 |
| **Lasso**                  | 8.41e+08 | 15674.4 | 0.8950 |
| **Linear Regression**      | 8.49e+08 | 15711.5 | 0.8941 |

------------------------------------------------------------
ğŸ” ConclusÃµes iniciais

â€¢ O XGBoost tunado apresentou o melhor desempenho geral, alcanÃ§ando o maior valor de RÂ² (0.924) e o menor erro (RMSE e MAE).  
â€¢ Modelos lineares (Ridge e Lasso) apresentaram resultados consistentes, mas inferiores aos modelos baseados em Ã¡rvores.  
â€¢ O Random Forest tambÃ©m se destacou, com desempenho prÃ³ximo ao do XGBoost.

------------------------------------------------------------
ğŸ§ª Tuning e otimizaÃ§Ã£o

Foi utilizada a biblioteca Optuna para otimizaÃ§Ã£o de hiperparÃ¢metros, com foco em maximizar o RÂ² e reduzir o RMSE.  
O experimento pode ser facilmente reproduzido ajustando os parÃ¢metros no notebook principal:

    experimento_house_prices.ipynb

------------------------------------------------------------
ğŸ§° Tecnologias utilizadas

â€¢ Python 3  
â€¢ Pandas, NumPy  
â€¢ Scikit-Learn  
â€¢ XGBoost  
â€¢ Optuna  
â€¢ Matplotlib / Seaborn  

------------------------------------------------------------
ğŸš€ ExecuÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
   git clone https://github.com/franciscocolatino/house-prices-ml-experimento.git
   cd house-prices-ml-experimento
```

2. Instale as dependÃªncias:
```bash
   pip install -r requirements.txt
```

3. Execute o notebook no Jupyter ou Google Colab:
```bash
   experimento_house_prices.ipynb
```

------------------------------------------------------------
ğŸ§© Testando o modelo treinado

Os modelos otimizados estÃ£o disponÃ­veis na pasta:

Resultados/
â”œâ”€â”€ best_random_forest_optuna.joblib
â””â”€â”€ best_xgboost_optuna.joblib

Para validar o modelo XGBoost otimizado, utilize o script testando_modelo.py.
Este arquivo cria um exemplo de imÃ³vel com caracterÃ­sticas conhecidas e gera a previsÃ£o de preÃ§o com base no modelo salvo.

```bash
   python testando_modelo.py
```

SaÃ­da esperada:

ğŸ’° PreÃ§o estimado do imÃ³vel: X

------------------------------------------------------------
âœï¸ Autores

Anderson da Silva Passos  
Francisco Colatino de Lima
JÃ´natas Duarte Vital Leite  

------------------------------------------------------------
Universidade Federal de Alagoas â€” 2025  
Disciplina: Planejamento de Experimentos
